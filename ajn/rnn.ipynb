{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "#from .module import Module\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are about to run on cpu, and this will likely run out       of memory. \n",
      " You can try setting batch_size=1 to reduce memory usage\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU if you have one\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU: \" + str(args.gpu_no))\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(args.gpu_no)\n",
    "else:\n",
    "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
    "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Problem 1\n",
    "\n",
    "    \n",
    "class Layer(nn.Module):\n",
    "  \n",
    "  def __init__(self, hidden_size):\n",
    "  \n",
    "    super(Layer, self).__init__()\n",
    "    self.hidden_size=hidden_size\n",
    "    self.fc = nn.Linear (self.hidden_size, self.hidden_size, bias = True)\n",
    "    self.rec = nn.Linear (self.hidden_size, self.hidden_size, bias = True)\n",
    "    \n",
    "\n",
    "class RNN(nn.Module): # Implement a stacked vanilla RNN with Tanh nonlinearities.\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "        \"\"\"\n",
    "        emb_size:     The number of units in the input embeddings\n",
    "        hidden_size:  The number of hidden units per layer\n",
    "        seq_len:      The length of the input sequences\n",
    "        vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
    "        num_layers:   The depth of the stack (i.e. the number of hidden layers at \n",
    "                      each time-step)\n",
    "        dp_keep_prob: The probability of *not* dropping out units in the \n",
    "                      non-recurrent connections.\n",
    "                      Do not apply dropout on recurrent connections.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size \n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dp_keep_prob = dp_keep_prob\n",
    "        \n",
    "        # Network definition\n",
    "\n",
    "        rnn_type = 'RNN_TANH'\n",
    "        self.drop = nn.Dropout(dp_keep_prob)\n",
    "        self.embedding = nn.Embedding (self.vocab_size, self.emb_size)\n",
    "        try:\n",
    "            nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "        except KeyError:\n",
    "            raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "        #self.rnn = nn.RNN(self.emb_size, self.num_layers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.rnn = nn.RNN(self.emb_size, self.hidden_size, self.num_layers, nonlinearity=nonlinearity, dropout=dp_keep_prob)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        # TODO ========================\n",
    "        # Initialize the embedding and output weights uniformly in the range [-0.1, 0.1]\n",
    "        # and output biases to 0 (in place). The embeddings should not use a bias vector.\n",
    "        # Initialize all other (i.e. recurrent and linear) weights AND biases uniformly \n",
    "        # in the range [-k, k] where k is the square root of 1/hidden_size\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        # TODO ========================\n",
    "        # initialize the hidden states to zero\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        # TODO ========================\n",
    "        # Compute the forward pass, using nested python for loops.\n",
    "        # The outer for loop should iterate over timesteps, and the \n",
    "        # inner for loop should iterate over hidden layers of the stack. \n",
    "        # \n",
    "        # Within these for loops, use the parameter tensors and/or nn.modules you \n",
    "        # created in __init__ to compute the recurrent updates according to the \n",
    "        # equations provided in the .tex of the assignment.\n",
    "        #\n",
    "        # Note that those equations are for a single hidden-layer RNN, not a stacked\n",
    "        # RNN. For a stacked RNN, the hidden states of the l-th layer are used as \n",
    "        # inputs to to the {l+1}-st layer (taking the place of the input sequence).\n",
    "\n",
    "        emb = self.drop(self.embedding(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "    \n",
    "    def generate(self, input, hidden, generated_seq_len):\n",
    "        # TODO ========================\n",
    "        # Compute the forward pass, as in the self.forward method (above).\n",
    "        # You'll probably want to copy substantial portions of that code here.\n",
    "        # \n",
    "        # We \"seed\" the generation by providing the first inputs.\n",
    "        # Subsequent inputs are generated by sampling from the output distribution, \n",
    "        # as described in the tex (Problem 5.3)\n",
    "        # Unlike for self.forward, you WILL need to apply the softmax activation \n",
    "        # function here in order to compute the parameters of the categorical \n",
    "        # distributions to be sampled from at each time-step.\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            - input: A mini-batch of input tokens (NOT sequences!)\n",
    "                            shape: (batch_size)\n",
    "            - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                            shape: (num_layers, batch_size, hidden_size)\n",
    "            - generated_seq_len: The length of the sequence to generate.\n",
    "                           Note that this can be different than the length used \n",
    "                           for training (self.seq_len)\n",
    "        Returns:\n",
    "            - Sampled sequences of tokens\n",
    "                        shape: (generated_seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        #return samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcess(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        #Create a 1-D tensor that contains the index of all the words in the file\n",
    "        rep_tensor = torch.LongTensor(tokens)\n",
    "        index = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    rep_tensor[index] = self.dictionary.word2idx[word]\n",
    "                    index += 1\n",
    "        #Find out how many batches we need   \n",
    "        num_batches = rep_tensor.shape[0] // batch_size   \n",
    "        print(\"# Batches: \", num_batches)\n",
    "        print(\"Batch size: \", batch_size)\n",
    "        #Remove the remainder (Filter out the ones that don't fit)\n",
    "        rep_tensor = rep_tensor[:num_batches*batch_size]\n",
    "        # return (batch_size,num_batches)\n",
    "        print(\"vocab_size: \", len(self.dictionary.idx2word))\n",
    "        rep_tensor = rep_tensor.view(batch_size, -1)\n",
    "        print(\"Rep_tensor shape: \", rep_tensor.shape)\n",
    "        return rep_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Batches:  250874\n",
      "Batch size:  20\n",
      "vocab_size:  50\n",
      "Rep_tensor shape:  torch.Size([20, 250874])\n"
     ]
    }
   ],
   "source": [
    "doc = TextProcess()\n",
    "train_data = doc.get_data('data/ptb.char.train.txt', 20)\n",
    "num_steps = 20\n",
    "epoch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = model.to(device)\n",
    "\n",
    "# LOSS FUNCTION\n",
    "vocab_size = 50\n",
    "hidden_dim = 100\n",
    "np.random.seed(10)\n",
    "#vocab_size=50\n",
    "emb_size=256\n",
    "batch_size=20\n",
    "hidden_size=200\n",
    "seq_len=50\n",
    "num_layers=2\n",
    "dp_keep_prob=0.35\n",
    "lr =1e-4\n",
    "# LEARNING RATE SCHEDULE    \n",
    "lr_decay_base = 1 / 1.15\n",
    "m_flat_lr = 14.0 # we will not touch lr for the first m_flat_lr epochs\n",
    "model = RNN(emb_size=emb_size, hidden_size=hidden_size, \n",
    "                seq_len=seq_len, batch_size=batch_size,\n",
    "                vocab_size=vocab_size, num_layers=num_layers, \n",
    "                dp_keep_prob=dp_keep_prob) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr)\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Running Main Loop ##########################\n",
      "Time: 2019-03-21 13:26:15, Timestep:0 Epoch [1/50], loss: 3.910272\n",
      "Time: 2019-03-21 13:26:23, Timestep:100 Epoch [1/50], loss: 3.186463\n",
      "Time: 2019-03-21 13:26:30, Timestep:200 Epoch [1/50], loss: 2.941118\n",
      "Time: 2019-03-21 13:26:37, Timestep:300 Epoch [1/50], loss: 2.814453\n",
      "Time: 2019-03-21 13:26:45, Timestep:400 Epoch [1/50], loss: 2.707091\n",
      "Time: 2019-03-21 13:26:52, Timestep:500 Epoch [1/50], loss: 2.610729\n",
      "Time: 2019-03-21 13:26:59, Timestep:600 Epoch [1/50], loss: 2.605049\n",
      "Time: 2019-03-21 13:27:06, Timestep:700 Epoch [1/50], loss: 2.554294\n",
      "Time: 2019-03-21 13:27:14, Timestep:800 Epoch [1/50], loss: 2.548042\n",
      "Time: 2019-03-21 13:27:22, Timestep:900 Epoch [1/50], loss: 2.519351\n",
      "Time: 2019-03-21 13:27:29, Timestep:1000 Epoch [1/50], loss: 2.490037\n",
      "Time: 2019-03-21 13:27:37, Timestep:1100 Epoch [1/50], loss: 2.453274\n",
      "Time: 2019-03-21 13:27:44, Timestep:1200 Epoch [1/50], loss: 2.459488\n",
      "Time: 2019-03-21 13:27:51, Timestep:1300 Epoch [1/50], loss: 2.476577\n",
      "Time: 2019-03-21 13:27:59, Timestep:1400 Epoch [1/50], loss: 2.441042\n",
      "Time: 2019-03-21 13:28:06, Timestep:1500 Epoch [1/50], loss: 2.428259\n",
      "Time: 2019-03-21 13:28:13, Timestep:1600 Epoch [1/50], loss: 2.525625\n",
      "Time: 2019-03-21 13:28:21, Timestep:1700 Epoch [1/50], loss: 2.478016\n",
      "Time: 2019-03-21 13:28:28, Timestep:1800 Epoch [1/50], loss: 2.537513\n",
      "Time: 2019-03-21 13:28:35, Timestep:1900 Epoch [1/50], loss: 2.519167\n",
      "Time: 2019-03-21 13:28:42, Timestep:2000 Epoch [1/50], loss: 2.503655\n",
      "Time: 2019-03-21 13:28:50, Timestep:2100 Epoch [1/50], loss: 2.360742\n",
      "Time: 2019-03-21 13:28:57, Timestep:2200 Epoch [1/50], loss: 2.336322\n",
      "Time: 2019-03-21 13:29:04, Timestep:2300 Epoch [1/50], loss: 2.502783\n",
      "Time: 2019-03-21 13:29:12, Timestep:2400 Epoch [1/50], loss: 2.496921\n",
      "Time: 2019-03-21 13:29:19, Timestep:2500 Epoch [1/50], loss: 2.520385\n",
      "Time: 2019-03-21 13:29:27, Timestep:2600 Epoch [1/50], loss: 2.572997\n",
      "Time: 2019-03-21 13:29:34, Timestep:2700 Epoch [1/50], loss: 2.511356\n",
      "Time: 2019-03-21 13:29:41, Timestep:2800 Epoch [1/50], loss: 2.448495\n",
      "Time: 2019-03-21 13:29:49, Timestep:2900 Epoch [1/50], loss: 2.434946\n",
      "Time: 2019-03-21 13:29:57, Timestep:3000 Epoch [1/50], loss: 2.393349\n",
      "Time: 2019-03-21 13:30:04, Timestep:3100 Epoch [1/50], loss: 2.349887\n",
      "Time: 2019-03-21 13:30:11, Timestep:3200 Epoch [1/50], loss: 2.426532\n",
      "Time: 2019-03-21 13:30:18, Timestep:3300 Epoch [1/50], loss: 2.435075\n",
      "Time: 2019-03-21 13:30:25, Timestep:3400 Epoch [1/50], loss: 2.450031\n",
      "Time: 2019-03-21 13:30:34, Timestep:3500 Epoch [1/50], loss: 2.448473\n",
      "Time: 2019-03-21 13:30:41, Timestep:3600 Epoch [1/50], loss: 2.409122\n",
      "Time: 2019-03-21 13:30:49, Timestep:3700 Epoch [1/50], loss: 2.418607\n",
      "Time: 2019-03-21 13:30:56, Timestep:3800 Epoch [1/50], loss: 2.465250\n",
      "Time: 2019-03-21 13:31:03, Timestep:3900 Epoch [1/50], loss: 2.413210\n",
      "Time: 2019-03-21 13:31:10, Timestep:4000 Epoch [1/50], loss: 2.547975\n",
      "Time: 2019-03-21 13:31:18, Timestep:4100 Epoch [1/50], loss: 2.411956\n",
      "Time: 2019-03-21 13:31:24, Timestep:4200 Epoch [1/50], loss: 2.345857\n",
      "Time: 2019-03-21 13:31:32, Timestep:4300 Epoch [1/50], loss: 2.411788\n",
      "Time: 2019-03-21 13:31:39, Timestep:4400 Epoch [1/50], loss: 2.484193\n",
      "Time: 2019-03-21 13:31:46, Timestep:4500 Epoch [1/50], loss: 2.331072\n",
      "Time: 2019-03-21 13:31:54, Timestep:4600 Epoch [1/50], loss: 2.467132\n",
      "Time: 2019-03-21 13:32:01, Timestep:4700 Epoch [1/50], loss: 2.452924\n",
      "Time: 2019-03-21 13:32:08, Timestep:4800 Epoch [1/50], loss: 2.380078\n",
      "Time: 2019-03-21 13:32:15, Timestep:4900 Epoch [1/50], loss: 2.363722\n",
      "Time: 2019-03-21 13:32:22, Timestep:5000 Epoch [1/50], loss: 2.373246\n",
      "Time: 2019-03-21 13:32:30, Timestep:5100 Epoch [1/50], loss: 2.440590\n",
      "Time: 2019-03-21 13:32:37, Timestep:5200 Epoch [1/50], loss: 2.380746\n",
      "Time: 2019-03-21 13:32:44, Timestep:5300 Epoch [1/50], loss: 2.386455\n",
      "Time: 2019-03-21 13:32:51, Timestep:5400 Epoch [1/50], loss: 2.409581\n",
      "Time: 2019-03-21 13:32:58, Timestep:5500 Epoch [1/50], loss: 2.463656\n",
      "Time: 2019-03-21 13:33:06, Timestep:5600 Epoch [1/50], loss: 2.445692\n",
      "Time: 2019-03-21 13:33:13, Timestep:5700 Epoch [1/50], loss: 2.327591\n",
      "Time: 2019-03-21 13:33:20, Timestep:5800 Epoch [1/50], loss: 2.458083\n",
      "Time: 2019-03-21 13:33:29, Timestep:5900 Epoch [1/50], loss: 2.443508\n",
      "Time: 2019-03-21 13:33:37, Timestep:6000 Epoch [1/50], loss: 2.409096\n",
      "Time: 2019-03-21 13:33:45, Timestep:6100 Epoch [1/50], loss: 2.480627\n",
      "Time: 2019-03-21 13:33:53, Timestep:6200 Epoch [1/50], loss: 2.385127\n",
      "Time: 2019-03-21 13:34:00, Timestep:6300 Epoch [1/50], loss: 2.373758\n",
      "Time: 2019-03-21 13:34:08, Timestep:6400 Epoch [1/50], loss: 2.422988\n",
      "Time: 2019-03-21 13:34:15, Timestep:6500 Epoch [1/50], loss: 2.431341\n",
      "Time: 2019-03-21 13:34:23, Timestep:6600 Epoch [1/50], loss: 2.437985\n",
      "Time: 2019-03-21 13:34:31, Timestep:6700 Epoch [1/50], loss: 2.296495\n",
      "Time: 2019-03-21 13:34:38, Timestep:6800 Epoch [1/50], loss: 2.434006\n",
      "Time: 2019-03-21 13:34:45, Timestep:6900 Epoch [1/50], loss: 2.552924\n",
      "Time: 2019-03-21 13:34:52, Timestep:7000 Epoch [1/50], loss: 2.393134\n",
      "Time: 2019-03-21 13:34:59, Timestep:7100 Epoch [1/50], loss: 2.468354\n",
      "Time: 2019-03-21 13:35:07, Timestep:7200 Epoch [1/50], loss: 2.489502\n",
      "Time: 2019-03-21 13:35:14, Timestep:7300 Epoch [1/50], loss: 2.481682\n",
      "Time: 2019-03-21 13:35:21, Timestep:7400 Epoch [1/50], loss: 2.429294\n",
      "Time: 2019-03-21 13:35:28, Timestep:7500 Epoch [1/50], loss: 2.443086\n",
      "Time: 2019-03-21 13:35:35, Timestep:7600 Epoch [1/50], loss: 2.481991\n",
      "Time: 2019-03-21 13:35:42, Timestep:7700 Epoch [1/50], loss: 2.401864\n",
      "Time: 2019-03-21 13:35:49, Timestep:7800 Epoch [1/50], loss: 2.415839\n",
      "Time: 2019-03-21 13:35:56, Timestep:7900 Epoch [1/50], loss: 2.397973\n",
      "Time: 2019-03-21 13:36:03, Timestep:8000 Epoch [1/50], loss: 2.338246\n",
      "Time: 2019-03-21 13:36:10, Timestep:8100 Epoch [1/50], loss: 2.477615\n",
      "Time: 2019-03-21 13:36:17, Timestep:8200 Epoch [1/50], loss: 2.388679\n",
      "Time: 2019-03-21 13:36:23, Timestep:8300 Epoch [1/50], loss: 2.444652\n",
      "Loss:  [2.4398813247680664]\n",
      "Time: 2019-03-21 13:36:28, Timestep:0 Epoch [2/50], loss: 2.437445\n",
      "Time: 2019-03-21 13:36:35, Timestep:100 Epoch [2/50], loss: 2.400493\n",
      "Time: 2019-03-21 13:36:43, Timestep:200 Epoch [2/50], loss: 2.364088\n",
      "Time: 2019-03-21 13:36:51, Timestep:300 Epoch [2/50], loss: 2.337934\n",
      "Time: 2019-03-21 13:36:59, Timestep:400 Epoch [2/50], loss: 2.369806\n",
      "Time: 2019-03-21 13:37:06, Timestep:500 Epoch [2/50], loss: 2.377538\n",
      "Time: 2019-03-21 13:37:14, Timestep:600 Epoch [2/50], loss: 2.451267\n",
      "Time: 2019-03-21 13:37:22, Timestep:700 Epoch [2/50], loss: 2.411264\n",
      "Time: 2019-03-21 13:37:30, Timestep:800 Epoch [2/50], loss: 2.436796\n",
      "Time: 2019-03-21 13:37:37, Timestep:900 Epoch [2/50], loss: 2.402376\n",
      "Time: 2019-03-21 13:37:45, Timestep:1000 Epoch [2/50], loss: 2.396699\n",
      "Time: 2019-03-21 13:37:52, Timestep:1100 Epoch [2/50], loss: 2.397779\n",
      "Time: 2019-03-21 13:37:59, Timestep:1200 Epoch [2/50], loss: 2.371460\n",
      "Time: 2019-03-21 13:38:07, Timestep:1300 Epoch [2/50], loss: 2.421471\n",
      "Time: 2019-03-21 13:38:15, Timestep:1400 Epoch [2/50], loss: 2.362243\n",
      "Time: 2019-03-21 13:38:22, Timestep:1500 Epoch [2/50], loss: 2.381621\n",
      "Time: 2019-03-21 13:38:29, Timestep:1600 Epoch [2/50], loss: 2.469478\n",
      "Time: 2019-03-21 13:38:38, Timestep:1700 Epoch [2/50], loss: 2.413592\n",
      "Time: 2019-03-21 13:38:46, Timestep:1800 Epoch [2/50], loss: 2.486561\n",
      "Time: 2019-03-21 13:38:55, Timestep:1900 Epoch [2/50], loss: 2.480489\n",
      "Time: 2019-03-21 13:39:04, Timestep:2000 Epoch [2/50], loss: 2.457293\n",
      "Time: 2019-03-21 13:39:11, Timestep:2100 Epoch [2/50], loss: 2.316763\n",
      "Time: 2019-03-21 13:39:19, Timestep:2200 Epoch [2/50], loss: 2.298920\n",
      "Time: 2019-03-21 13:39:27, Timestep:2300 Epoch [2/50], loss: 2.491194\n",
      "Time: 2019-03-21 13:39:34, Timestep:2400 Epoch [2/50], loss: 2.443711\n",
      "Time: 2019-03-21 13:39:42, Timestep:2500 Epoch [2/50], loss: 2.480739\n",
      "Time: 2019-03-21 13:39:50, Timestep:2600 Epoch [2/50], loss: 2.514306\n",
      "Time: 2019-03-21 13:39:58, Timestep:2700 Epoch [2/50], loss: 2.460972\n",
      "Time: 2019-03-21 13:40:05, Timestep:2800 Epoch [2/50], loss: 2.391938\n",
      "Time: 2019-03-21 13:40:12, Timestep:2900 Epoch [2/50], loss: 2.424990\n",
      "Time: 2019-03-21 13:40:19, Timestep:3000 Epoch [2/50], loss: 2.338244\n",
      "Time: 2019-03-21 13:40:27, Timestep:3100 Epoch [2/50], loss: 2.307283\n",
      "Time: 2019-03-21 13:40:34, Timestep:3200 Epoch [2/50], loss: 2.414309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2019-03-21 13:40:41, Timestep:3300 Epoch [2/50], loss: 2.425675\n",
      "Time: 2019-03-21 13:40:48, Timestep:3400 Epoch [2/50], loss: 2.415289\n",
      "Time: 2019-03-21 13:40:56, Timestep:3500 Epoch [2/50], loss: 2.398203\n",
      "Time: 2019-03-21 13:41:04, Timestep:3600 Epoch [2/50], loss: 2.369802\n",
      "Time: 2019-03-21 13:41:13, Timestep:3700 Epoch [2/50], loss: 2.397220\n",
      "Time: 2019-03-21 13:41:21, Timestep:3800 Epoch [2/50], loss: 2.446721\n",
      "Time: 2019-03-21 13:41:29, Timestep:3900 Epoch [2/50], loss: 2.400224\n",
      "Time: 2019-03-21 13:41:37, Timestep:4000 Epoch [2/50], loss: 2.504844\n",
      "Time: 2019-03-21 13:41:46, Timestep:4100 Epoch [2/50], loss: 2.384525\n",
      "Time: 2019-03-21 13:41:54, Timestep:4200 Epoch [2/50], loss: 2.309197\n",
      "Time: 2019-03-21 13:42:01, Timestep:4300 Epoch [2/50], loss: 2.388321\n",
      "Time: 2019-03-21 13:42:09, Timestep:4400 Epoch [2/50], loss: 2.456021\n",
      "Time: 2019-03-21 13:42:17, Timestep:4500 Epoch [2/50], loss: 2.299762\n",
      "Time: 2019-03-21 13:42:24, Timestep:4600 Epoch [2/50], loss: 2.445347\n",
      "Time: 2019-03-21 13:42:31, Timestep:4700 Epoch [2/50], loss: 2.409538\n",
      "Time: 2019-03-21 13:42:38, Timestep:4800 Epoch [2/50], loss: 2.373031\n",
      "Time: 2019-03-21 13:42:45, Timestep:4900 Epoch [2/50], loss: 2.347017\n",
      "Time: 2019-03-21 13:42:52, Timestep:5000 Epoch [2/50], loss: 2.344376\n",
      "Time: 2019-03-21 13:42:59, Timestep:5100 Epoch [2/50], loss: 2.400419\n",
      "Time: 2019-03-21 13:43:06, Timestep:5200 Epoch [2/50], loss: 2.363454\n",
      "Time: 2019-03-21 13:43:15, Timestep:5300 Epoch [2/50], loss: 2.404826\n",
      "Time: 2019-03-21 13:43:21, Timestep:5400 Epoch [2/50], loss: 2.394533\n",
      "Time: 2019-03-21 13:43:29, Timestep:5500 Epoch [2/50], loss: 2.418130\n",
      "Time: 2019-03-21 13:43:37, Timestep:5600 Epoch [2/50], loss: 2.393608\n",
      "Time: 2019-03-21 13:43:45, Timestep:5700 Epoch [2/50], loss: 2.310334\n",
      "Time: 2019-03-21 13:43:51, Timestep:5800 Epoch [2/50], loss: 2.451775\n",
      "Time: 2019-03-21 13:43:59, Timestep:5900 Epoch [2/50], loss: 2.406093\n",
      "Time: 2019-03-21 13:44:07, Timestep:6000 Epoch [2/50], loss: 2.378439\n",
      "Time: 2019-03-21 13:44:16, Timestep:6100 Epoch [2/50], loss: 2.464948\n",
      "Time: 2019-03-21 13:44:25, Timestep:6200 Epoch [2/50], loss: 2.372967\n",
      "Time: 2019-03-21 13:44:33, Timestep:6300 Epoch [2/50], loss: 2.389054\n",
      "Time: 2019-03-21 13:44:41, Timestep:6400 Epoch [2/50], loss: 2.406111\n",
      "Time: 2019-03-21 13:44:48, Timestep:6500 Epoch [2/50], loss: 2.424660\n",
      "Time: 2019-03-21 13:44:55, Timestep:6600 Epoch [2/50], loss: 2.441026\n",
      "Time: 2019-03-21 13:45:03, Timestep:6700 Epoch [2/50], loss: 2.293307\n",
      "Time: 2019-03-21 13:45:10, Timestep:6800 Epoch [2/50], loss: 2.402297\n",
      "Time: 2019-03-21 13:45:17, Timestep:6900 Epoch [2/50], loss: 2.560496\n",
      "Time: 2019-03-21 13:45:24, Timestep:7000 Epoch [2/50], loss: 2.395486\n",
      "Time: 2019-03-21 13:45:32, Timestep:7100 Epoch [2/50], loss: 2.477221\n",
      "Time: 2019-03-21 13:45:40, Timestep:7200 Epoch [2/50], loss: 2.478301\n",
      "Time: 2019-03-21 13:45:47, Timestep:7300 Epoch [2/50], loss: 2.479024\n",
      "Time: 2019-03-21 13:45:54, Timestep:7400 Epoch [2/50], loss: 2.452960\n",
      "Time: 2019-03-21 13:46:01, Timestep:7500 Epoch [2/50], loss: 2.424503\n",
      "Time: 2019-03-21 13:46:09, Timestep:7600 Epoch [2/50], loss: 2.470247\n",
      "Time: 2019-03-21 13:46:16, Timestep:7700 Epoch [2/50], loss: 2.385148\n",
      "Time: 2019-03-21 13:46:24, Timestep:7800 Epoch [2/50], loss: 2.409245\n",
      "Time: 2019-03-21 13:46:32, Timestep:7900 Epoch [2/50], loss: 2.385166\n",
      "Time: 2019-03-21 13:46:39, Timestep:8000 Epoch [2/50], loss: 2.333150\n",
      "Time: 2019-03-21 13:46:46, Timestep:8100 Epoch [2/50], loss: 2.471438\n",
      "Time: 2019-03-21 13:46:54, Timestep:8200 Epoch [2/50], loss: 2.368278\n",
      "Time: 2019-03-21 13:47:02, Timestep:8300 Epoch [2/50], loss: 2.437677\n",
      "Loss:  [2.4398813247680664, 2.423025608062744]\n",
      "Time: 2019-03-21 13:47:07, Timestep:0 Epoch [3/50], loss: 2.437389\n",
      "Time: 2019-03-21 13:47:14, Timestep:100 Epoch [3/50], loss: 2.379634\n",
      "Time: 2019-03-21 13:47:21, Timestep:200 Epoch [3/50], loss: 2.331145\n",
      "Time: 2019-03-21 13:47:28, Timestep:300 Epoch [3/50], loss: 2.334696\n",
      "Time: 2019-03-21 13:47:37, Timestep:400 Epoch [3/50], loss: 2.378412\n",
      "Time: 2019-03-21 13:47:45, Timestep:500 Epoch [3/50], loss: 2.365325\n",
      "Time: 2019-03-21 13:47:54, Timestep:600 Epoch [3/50], loss: 2.463780\n",
      "Time: 2019-03-21 13:48:02, Timestep:700 Epoch [3/50], loss: 2.385110\n",
      "Time: 2019-03-21 13:48:10, Timestep:800 Epoch [3/50], loss: 2.443087\n",
      "Time: 2019-03-21 13:48:18, Timestep:900 Epoch [3/50], loss: 2.404514\n",
      "Time: 2019-03-21 13:48:25, Timestep:1000 Epoch [3/50], loss: 2.406184\n",
      "Time: 2019-03-21 13:48:33, Timestep:1100 Epoch [3/50], loss: 2.386411\n",
      "Time: 2019-03-21 13:48:40, Timestep:1200 Epoch [3/50], loss: 2.366105\n",
      "Time: 2019-03-21 13:48:47, Timestep:1300 Epoch [3/50], loss: 2.400702\n",
      "Time: 2019-03-21 13:48:54, Timestep:1400 Epoch [3/50], loss: 2.380642\n",
      "Time: 2019-03-21 13:49:02, Timestep:1500 Epoch [3/50], loss: 2.370430\n",
      "Time: 2019-03-21 13:49:09, Timestep:1600 Epoch [3/50], loss: 2.465131\n",
      "Time: 2019-03-21 13:49:16, Timestep:1700 Epoch [3/50], loss: 2.400334\n",
      "Time: 2019-03-21 13:49:23, Timestep:1800 Epoch [3/50], loss: 2.475945\n",
      "Time: 2019-03-21 13:49:30, Timestep:1900 Epoch [3/50], loss: 2.473178\n",
      "Time: 2019-03-21 13:49:37, Timestep:2000 Epoch [3/50], loss: 2.427360\n",
      "Time: 2019-03-21 13:49:45, Timestep:2100 Epoch [3/50], loss: 2.297903\n",
      "Time: 2019-03-21 13:49:53, Timestep:2200 Epoch [3/50], loss: 2.278470\n",
      "Time: 2019-03-21 13:50:00, Timestep:2300 Epoch [3/50], loss: 2.455180\n",
      "Time: 2019-03-21 13:50:08, Timestep:2400 Epoch [3/50], loss: 2.429935\n",
      "Time: 2019-03-21 13:50:15, Timestep:2500 Epoch [3/50], loss: 2.450805\n",
      "Time: 2019-03-21 13:50:24, Timestep:2600 Epoch [3/50], loss: 2.528424\n",
      "Time: 2019-03-21 13:50:33, Timestep:2700 Epoch [3/50], loss: 2.460547\n",
      "Time: 2019-03-21 13:50:41, Timestep:2800 Epoch [3/50], loss: 2.380291\n",
      "Time: 2019-03-21 13:50:49, Timestep:2900 Epoch [3/50], loss: 2.394854\n",
      "Time: 2019-03-21 13:50:57, Timestep:3000 Epoch [3/50], loss: 2.331730\n",
      "Time: 2019-03-21 13:51:05, Timestep:3100 Epoch [3/50], loss: 2.342706\n",
      "Time: 2019-03-21 13:51:13, Timestep:3200 Epoch [3/50], loss: 2.403138\n",
      "Time: 2019-03-21 13:51:20, Timestep:3300 Epoch [3/50], loss: 2.416858\n",
      "Time: 2019-03-21 13:51:28, Timestep:3400 Epoch [3/50], loss: 2.409780\n",
      "Time: 2019-03-21 13:51:37, Timestep:3500 Epoch [3/50], loss: 2.422276\n",
      "Time: 2019-03-21 13:51:45, Timestep:3600 Epoch [3/50], loss: 2.364515\n",
      "Time: 2019-03-21 13:51:52, Timestep:3700 Epoch [3/50], loss: 2.416243\n",
      "Time: 2019-03-21 13:52:00, Timestep:3800 Epoch [3/50], loss: 2.438287\n",
      "Time: 2019-03-21 13:52:08, Timestep:3900 Epoch [3/50], loss: 2.402255\n",
      "Time: 2019-03-21 13:52:16, Timestep:4000 Epoch [3/50], loss: 2.515472\n",
      "Time: 2019-03-21 13:52:23, Timestep:4100 Epoch [3/50], loss: 2.379256\n",
      "Time: 2019-03-21 13:52:31, Timestep:4200 Epoch [3/50], loss: 2.303951\n",
      "Time: 2019-03-21 13:52:39, Timestep:4300 Epoch [3/50], loss: 2.385541\n",
      "Time: 2019-03-21 13:52:48, Timestep:4400 Epoch [3/50], loss: 2.442876\n",
      "Time: 2019-03-21 13:52:55, Timestep:4500 Epoch [3/50], loss: 2.302939\n",
      "Time: 2019-03-21 13:53:04, Timestep:4600 Epoch [3/50], loss: 2.444559\n",
      "Time: 2019-03-21 13:53:12, Timestep:4700 Epoch [3/50], loss: 2.439995\n",
      "Time: 2019-03-21 13:53:19, Timestep:4800 Epoch [3/50], loss: 2.360333\n",
      "Time: 2019-03-21 13:53:26, Timestep:4900 Epoch [3/50], loss: 2.346050\n",
      "Time: 2019-03-21 13:53:34, Timestep:5000 Epoch [3/50], loss: 2.340624\n",
      "Time: 2019-03-21 13:53:41, Timestep:5100 Epoch [3/50], loss: 2.374357\n",
      "Time: 2019-03-21 13:53:48, Timestep:5200 Epoch [3/50], loss: 2.364097\n",
      "Time: 2019-03-21 13:53:55, Timestep:5300 Epoch [3/50], loss: 2.412352\n",
      "Time: 2019-03-21 13:54:03, Timestep:5400 Epoch [3/50], loss: 2.407329\n",
      "Time: 2019-03-21 13:54:11, Timestep:5500 Epoch [3/50], loss: 2.426983\n",
      "Time: 2019-03-21 13:54:18, Timestep:5600 Epoch [3/50], loss: 2.394425\n",
      "Time: 2019-03-21 13:54:25, Timestep:5700 Epoch [3/50], loss: 2.307707\n",
      "Time: 2019-03-21 13:54:33, Timestep:5800 Epoch [3/50], loss: 2.443280\n",
      "Time: 2019-03-21 13:54:42, Timestep:5900 Epoch [3/50], loss: 2.414495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-33c905c9a643>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n########## Running Main Loop ##########################\")\n",
    "loss_arr = []\n",
    "num_epochs = 50\n",
    "\n",
    "#===============================ADDED\n",
    "num_examples_seen = 0\n",
    "num_steps=30\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    states=(torch.zeros(num_layers,num_steps,hidden_size))\n",
    "    #states=(torch.zeros(num_layers,batch_size,hidden_size))\n",
    "    for i in range (0, train_data.size(1)-num_steps,num_steps):\n",
    "        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        inputs = train_data[:, i:(i+num_steps)]\n",
    "        targets = train_data[:, (i+1):(i+1)+num_steps]\n",
    "        outputs,_ = model(inputs,states)\n",
    "        loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), targets.reshape(-1))  \n",
    "        #Back propagation\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "        \n",
    "        step=(i+1) // num_steps\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print (\"Time: {}, Timestep:{} Epoch [{}/{}], loss: {:4f}\".format(time,step,epoch+1,num_epochs,loss.item()))\n",
    "    loss_arr.append(loss.item())\n",
    "    print(\"Loss: \", loss_arr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
