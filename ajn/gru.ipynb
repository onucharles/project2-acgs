{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "#from .module import Module\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are about to run on cpu, and this will likely run out       of memory. \n",
      " You can try setting batch_size=1 to reduce memory usage\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU if you have one\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU: \" + str(args.gpu_no))\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.set_device(args.gpu_no)\n",
    "else:\n",
    "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
    "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def predict(self, x):\n",
    "        exp_scores = np.exp(x)\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        return -np.log(probs[y])\n",
    "\n",
    "    def diff(self, x, y):\n",
    "        probs = self.predict(x)\n",
    "        probs[y] -= 1.0\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# Problem 1\n",
    "\n",
    "    \n",
    "class Layer(nn.Module):\n",
    "  \n",
    "  def __init__(self, hidden_size):\n",
    "  \n",
    "    super(Layer, self).__init__()\n",
    "    self.hidden_size=hidden_size\n",
    "    self.fc = nn.Linear (self.hidden_size, self.hidden_size, bias = True)\n",
    "    self.rec = nn.Linear (self.hidden_size, self.hidden_size, bias = True)\n",
    "    \n",
    "\n",
    "class RNN(nn.Module): # Implement GRU.\n",
    "    \n",
    "    def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size, num_layers, dp_keep_prob):\n",
    "        \"\"\"\n",
    "        emb_size:     The number of units in the input embeddings\n",
    "        hidden_size:  The number of hidden units per layer\n",
    "        seq_len:      The length of the input sequences\n",
    "        vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
    "        num_layers:   The depth of the stack (i.e. the number of hidden layers at \n",
    "                      each time-step)\n",
    "        dp_keep_prob: The probability of *not* dropping out units in the \n",
    "                      non-recurrent connections.\n",
    "                      Do not apply dropout on recurrent connections.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.emb_size = emb_size \n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dp_keep_prob = dp_keep_prob\n",
    "        \n",
    "        # Network definition\n",
    "        rnn_type = 'GRU'\n",
    "        self.drop = nn.Dropout(dp_keep_prob)\n",
    "        self.embedding = nn.Embedding (self.vocab_size, self.emb_size)\n",
    "        self.rnn = getattr(nn, rnn_type)(self.emb_size, self.hidden_size, self.num_layers, dropout=dp_keep_prob)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.init_weights() \n",
    "\n",
    "    def init_weights(self):\n",
    "        # TODO ========================\n",
    "        # Initialize the embedding and output weights uniformly in the range [-0.1, 0.1]\n",
    "        # and output biases to 0 (in place). The embeddings should not use a bias vector.\n",
    "        # Initialize all other (i.e. recurrent and linear) weights AND biases uniformly \n",
    "        # in the range [-k, k] where k is the square root of 1/hidden_size\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        # TODO ========================\n",
    "        # initialize the hidden states to zero\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        # TODO ========================\n",
    "        # Compute the forward pass, using nested python for loops.\n",
    "        # The outer for loop should iterate over timesteps, and the \n",
    "        # inner for loop should iterate over hidden layers of the stack. \n",
    "        # \n",
    "        # Within these for loops, use the parameter tensors and/or nn.modules you \n",
    "        # created in __init__ to compute the recurrent updates according to the \n",
    "        # equations provided in the .tex of the assignment.\n",
    "        #\n",
    "        # Note that those equations are for a single hidden-layer RNN, not a stacked\n",
    "        # RNN. For a stacked RNN, the hidden states of the l-th layer are used as \n",
    "        # inputs to to the {l+1}-st layer (taking the place of the input sequence).\n",
    "\n",
    "        emb = self.drop(self.embedding(inputs))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "    \n",
    "    def generate(self, input, hidden, generated_seq_len):\n",
    "        # TODO ========================\n",
    "        # Compute the forward pass, as in the self.forward method (above).\n",
    "        # You'll probably want to copy substantial portions of that code here.\n",
    "        # \n",
    "        # We \"seed\" the generation by providing the first inputs.\n",
    "        # Subsequent inputs are generated by sampling from the output distribution, \n",
    "        # as described in the tex (Problem 5.3)\n",
    "        # Unlike for self.forward, you WILL need to apply the softmax activation \n",
    "        # function here in order to compute the parameters of the categorical \n",
    "        # distributions to be sampled from at each time-step.\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            - input: A mini-batch of input tokens (NOT sequences!)\n",
    "                            shape: (batch_size)\n",
    "            - hidden: The initial hidden states for every layer of the stacked RNN.\n",
    "                            shape: (num_layers, batch_size, hidden_size)\n",
    "            - generated_seq_len: The length of the sequence to generate.\n",
    "                           Note that this can be different than the length used \n",
    "                           for training (self.seq_len)\n",
    "        Returns:\n",
    "            - Sampled sequences of tokens\n",
    "                        shape: (generated_seq_len, batch_size)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        #return samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcess(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words: \n",
    "                    self.dictionary.add_word(word)  \n",
    "        #Create a 1-D tensor that contains the index of all the words in the file\n",
    "        rep_tensor = torch.LongTensor(tokens)\n",
    "        index = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    rep_tensor[index] = self.dictionary.word2idx[word]\n",
    "                    index += 1\n",
    "        #Find out how many batches we need   \n",
    "        num_batches = rep_tensor.shape[0] // batch_size   \n",
    "        print(\"# Batches: \", num_batches)\n",
    "        print(\"Batch size: \", batch_size)\n",
    "        #Remove the remainder (Filter out the ones that don't fit)\n",
    "        rep_tensor = rep_tensor[:num_batches*batch_size]\n",
    "        # return (batch_size,num_batches)\n",
    "        print(\"vocab_size: \", len(self.dictionary.idx2word))\n",
    "        rep_tensor = rep_tensor.view(batch_size, -1)\n",
    "        print(\"Rep_tensor shape: \", rep_tensor.shape)\n",
    "        return rep_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Batches:  250874\n",
      "Batch size:  20\n",
      "vocab_size:  50\n",
      "Rep_tensor shape:  torch.Size([20, 250874])\n"
     ]
    }
   ],
   "source": [
    "doc = TextProcess()\n",
    "train_data = doc.get_data('data/ptb.char.train.txt', 20)\n",
    "num_steps = 20\n",
    "epoch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = 50\n",
    "hidden_dim = 100\n",
    "np.random.seed(10)\n",
    "#vocab_size=50\n",
    "emb_size=256\n",
    "batch_size=20\n",
    "hidden_size=200\n",
    "seq_len=50\n",
    "num_layers=2\n",
    "dp_keep_prob=0.35\n",
    "lr =1e-4\n",
    "# LEARNING RATE SCHEDULE    \n",
    "lr_decay_base = 1 / 1.15\n",
    "m_flat_lr = 14.0 # we will not touch lr for the first m_flat_lr epochs\n",
    "model = RNN(emb_size=emb_size, hidden_size=hidden_size, \n",
    "                seq_len=seq_len, batch_size=batch_size,\n",
    "                vocab_size=vocab_size, num_layers=num_layers, \n",
    "                dp_keep_prob=dp_keep_prob) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Running Main Loop ##########################\n",
      "Time: 2019-03-21 13:19:34, Timestep:0 Epoch [1/50], loss: 2.422590\n",
      "Time: 2019-03-21 13:19:40, Timestep:100 Epoch [1/50], loss: 2.377115\n",
      "Time: 2019-03-21 13:19:48, Timestep:200 Epoch [1/50], loss: 2.326538\n",
      "Time: 2019-03-21 13:19:56, Timestep:300 Epoch [1/50], loss: 2.309144\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n########## Running Main Loop ##########################\")\n",
    "loss_arr = []\n",
    "num_epochs = 50\n",
    "\n",
    "#===============================ADDED\n",
    "num_examples_seen = 0\n",
    "num_steps=30\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    states=(torch.zeros(num_layers,num_steps,hidden_size))\n",
    "    #states=(torch.zeros(num_layers,batch_size,hidden_size))\n",
    "    for i in range (0, train_data.size(1)-num_steps,num_steps):\n",
    "        time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        inputs = train_data[:, i:(i+num_steps)]\n",
    "        targets = train_data[:, (i+1):(i+1)+num_steps]\n",
    "        outputs,_ = model(inputs,states)\n",
    "        loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), targets.reshape(-1))  \n",
    "        #Back propagation\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()\n",
    "        \n",
    "        step=(i+1) // num_steps\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print (\"Time: {}, Timestep:{} Epoch [{}/{}], loss: {:4f}\".format(time,step,epoch+1,num_epochs,loss.item()))\n",
    "    loss_arr.append(loss.item())\n",
    "    print(\"Loss: \", loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
